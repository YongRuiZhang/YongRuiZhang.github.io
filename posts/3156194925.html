<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>MapReduce | 秋白's Blog</title><meta name="author" content="秋白"><meta name="copyright" content="秋白"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="分布式运算程序">
<meta property="og:type" content="article">
<meta property="og:title" content="MapReduce">
<meta property="og:url" content="http://yongruizhang.cn/posts/3156194925.html">
<meta property="og:site_name" content="秋白&#39;s Blog">
<meta property="og:description" content="分布式运算程序">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://th.bing.com/th/id/R.d60c7923a45b06826278a8d3fa6cd1cc?rik=N6M9fQqCw7Ijsg&riu=http%3a%2f%2ferrequeerre.es%2fwp-content%2fuploads%2f2017%2f08%2fHadoop-Map-Reduce.png&ehk=4zYF4J3scSG5mcQ%2fFQqylAoWYreGhEr7IFpmCu8lXnc%3d&risl=&pid=ImgRaw&r=0">
<meta property="article:published_time" content="2023-05-14T13:36:03.000Z">
<meta property="article:modified_time" content="2023-05-17T09:31:00.067Z">
<meta property="article:author" content="秋白">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://th.bing.com/th/id/R.d60c7923a45b06826278a8d3fa6cd1cc?rik=N6M9fQqCw7Ijsg&riu=http%3a%2f%2ferrequeerre.es%2fwp-content%2fuploads%2f2017%2f08%2fHadoop-Map-Reduce.png&ehk=4zYF4J3scSG5mcQ%2fFQqylAoWYreGhEr7IFpmCu8lXnc%3d&risl=&pid=ImgRaw&r=0"><link rel="shortcut icon" href="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/favicon.png"><link rel="canonical" href="http://yongruizhang.cn/posts/3156194925.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":500},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MapReduce',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-17 17:31:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><script src="https://cdn.jsdelivr.net/npm/echarts@5.4.1/dist/echarts.min.js"></script><link rel="stylesheet" href="/css/twikoo1.css"><link rel="stylesheet" href="/css/footer1.css"><link rel="apple-touch-icon" href="/img/favicon.jpeg"><meta name="apple-mobile-web-app-title" content="秋白's Blog"><link rel="bookmark" href="/img/favicon.jpeg"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/favicon.jpeg" ><link rel="stylesheet" href="/css/footer_bg.css"><link rel="stylesheet" href="/css/nav1.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/favicon.png" onerror="onerror=null;src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">96</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-folder-open"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://th.bing.com/th/id/R.d60c7923a45b06826278a8d3fa6cd1cc?rik=N6M9fQqCw7Ijsg&amp;riu=http%3a%2f%2ferrequeerre.es%2fwp-content%2fuploads%2f2017%2f08%2fHadoop-Map-Reduce.png&amp;ehk=4zYF4J3scSG5mcQ%2fFQqylAoWYreGhEr7IFpmCu8lXnc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0')"><nav id="nav"><span id="blog-info"><a href="/" title="秋白's Blog"><img class="site-icon" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/favicon.png"/><span class="site-name">秋白's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-regular fa-folder-open"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MapReduce</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-14T13:36:03.000Z" title="发表于 2023-05-14 21:36:03">2023-05-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-17T09:31:00.067Z" title="更新于 2023-05-17 17:31:00">2023-05-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">大数据开发</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/">Hadoop</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/%E6%8A%80%E6%9C%AF/">技术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>69分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="MapReduce"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post" style="background: white; backdrop-filter: blur(5px); opacity: 0.98"><article class="post-content" id="article-container"><details>此笔记是我学习 B站 尚硅谷相关课程的学习笔记</details>

<blockquote>
<p><strong>项目源码：</strong><a target="_blank" rel="noopener" href="https://github.com/YongRuiZhang/hadoop_study_demo">https://github.com/YongRuiZhang/hadoop_study_demo</a></p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817181510929.png" alt="image-20220817181510929"  style="zoom:33%;" /></p>
<h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><h2 id="1-1-、定义"><a href="#1-1-、定义" class="headerlink" title="1.1 、定义"></a>1.1 、定义</h2><p>MapReduce 是一个<strong>分布式运算程序</strong>的<strong>编程框架</strong>，是用户开发“基于 Hadoop 的数据分析应用”的核心框架。</p>
<p>MapReduce 核心功能是<strong>将用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布式运算程序</strong>，并发运行在一个 Hadoop 集群上。</p>
<h2 id="1-2、优缺点"><a href="#1-2、优缺点" class="headerlink" title="1.2、优缺点"></a>1.2、优缺点</h2><h3 id="1-2-1、优点"><a href="#1-2-1、优点" class="headerlink" title="1.2.1、优点"></a>1.2.1、优点</h3><ol>
<li><p>易于编程</p>
<p> <strong>它简单的实现一些接口，就可以完成一个分布式程序</strong>，这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一 样的。就是因为这个特点使得 MapReduce 编程变得非常流行。</p>
</li>
<li><p>良好的扩展性</p>
<p> 当你的计算资源不能得到满足的时候，你可以通过<strong>简单的增加机器</strong>来扩展它的计算能力。</p>
</li>
<li><p>高容错性</p>
<p> MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。比如<strong>其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行， 不至于这个任务运行失败</strong>，而且这个过程<strong>不需要人工参与</strong>，而完全是由 Hadoop 内部完成的。</p>
</li>
<li><p>适合 <strong>PB 级</strong>以上海量数据的<strong>离线处理</strong></p>
<p> 可以实现上千台服务器集群并发工作，提供数据处理能力。</p>
</li>
</ol>
<h3 id="1-2-2、缺点"><a href="#1-2-2、缺点" class="headerlink" title="1.2.2、缺点"></a>1.2.2、缺点</h3><ol>
<li><p>不擅长实时计算</p>
<p> MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。</p>
</li>
<li><p>不擅长流式计算</p>
<p> 流式计算的输入数据是动态的，而 MapReduce 的<strong>输入数据集是静态的</strong>，不能动态变化。这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。</p>
</li>
<li><p>）不擅长 DAG（有向无环图）计算</p>
<p> 多个应用程序<strong>存在依赖关系</strong>，后一个应用程序的输入为前一个的输出。在这种情况下， MapReduce 并不是不能做，而是使用后，<strong>每个 MapReduce 作业的输出结果都会写入到磁盘， 会造成大量的磁盘 IO，导致性能非常的低下。</strong></p>
</li>
</ol>
<h2 id="1-3、核心思想"><a href="#1-3、核心思想" class="headerlink" title="1.3、核心思想"></a>1.3、核心思想</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817192236901.png" alt="image-20220817192236901"></p>
<ol>
<li>分布式的运算程序往往需要分成至少 2 个阶段。</li>
<li>第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。</li>
<li>第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段 的所有 MapTask 并发实例的输出。</li>
<li>MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业 务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。</li>
</ol>
<p>总结：分析 WordCount 数据流走向深入理解 MapReduce 核心思想。</p>
<p>Map阶段是切分操作，Reduce阶段是计算操作</p>
<p>==WordCount相当于就是HelloWorld程序一样==</p>
<h2 id="1-4、MapReduce-进程"><a href="#1-4、MapReduce-进程" class="headerlink" title="1.4、MapReduce 进程"></a>1.4、MapReduce 进程</h2><p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程： </p>
<ol>
<li>MrAppMaster：负责整个程序的过程调度及状态协调。 </li>
<li>MapTask：负责 Map 阶段的整个数据处理流程。 </li>
<li>ReduceTask：负责 Reduce 阶段的整个数据处理流程。</li>
</ol>
<h2 id="1-5、官方-WordCount-源码"><a href="#1-5、官方-WordCount-源码" class="headerlink" title="1.5、官方 WordCount 源码"></a>1.5、官方 WordCount 源码</h2><p>hadoop包里面由/share文件夹，用来存储官方的一些jar包，其中在<code>/share/hadoop/mapreduce</code>下存了一些mapreduce有关的jar包：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817193846283.png" alt="image-20220817193846283"></p>
<p>其中 WordCount 就在hadoop-mapreduce-example-3.3.3.jar包中。我们将这个包传到Windows。然后反编译源码。</p>
<p>==采用反编译工具反编译源码，发现 WordCount 案例有 Map 类、Reduce 类和驱动类。且数据的类型是 Hadoop 自身封装的序列化类型。==</p>
<h2 id="1-6、重用数据序列化类型"><a href="#1-6、重用数据序列化类型" class="headerlink" title="1.6、重用数据序列化类型"></a>1.6、重用数据序列化类型</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable 类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Integer / int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td><span style="color:red">String</span></td>
<td><span style="color:red">Text</span></td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
<tr>
<td>Null</td>
<td>NullWritable</td>
</tr>
</tbody>
</table>
</div>
<p>除了标红的一组之外，都是在在原本的类型后面加Writable。</p>
<h2 id="1-7、MapReduce-编程规范"><a href="#1-7、MapReduce-编程规范" class="headerlink" title="1.7、MapReduce 编程规范"></a>1.7、MapReduce 编程规范</h2><blockquote>
<p>用户编写的程序分成三个部分：<a href="#1、Mapper">Mapper</a>、<a href="#2、Reducer">Reducer</a>、<a href="#3、Driver">Driver</a>。</p>
</blockquote>
<h3 id="1、Mapper"><a href="#1、Mapper" class="headerlink" title="1、Mapper"></a>1、Mapper</h3><ol>
<li>用户自定义的Mapper要继承自己的父类</li>
<li>Mapper的输入数据是KV对的形式（KV的类型可自定义）</li>
<li>Mapper中的业务逻辑写在map（）方法中</li>
<li>Mapper的输出数据是KV对的形式（KV的类型可自定义）</li>
<li>map()方法（MapTask进程）对每一个<K,V>调用一次</li>
</ol>
<h3 id="2、Reducer"><a href="#2、Reducer" class="headerlink" title="2、Reducer"></a>2、Reducer</h3><ol>
<li>用户自定义的Reducer要继承之际的父类</li>
<li>Reducer的输入数据对应Mapper的输出数据类型，也是KV</li>
<li>Reducer的业务逻辑写在reduce()方法中</li>
<li>ReduceTask进程对每一组相同k的组调用一次reduce()方法</li>
</ol>
<h3 id="3、Driver"><a href="#3、Driver" class="headerlink" title="3、Driver"></a>3、Driver</h3><p>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是：封装了MapReduce程序相关运行参数的==job对象==</p>
<h2 id="1-8、WordCount案例实操"><a href="#1-8、WordCount案例实操" class="headerlink" title="1.8、WordCount案例实操"></a>1.8、WordCount案例实操</h2><h3 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h3><ol>
<li><p>需求</p>
<p> 在给定的文本文件中统计输出每一个单词出现的总次数</p>
</li>
<li><p>需求分析</p>
<p> 按照 MapReduce 编程规范，分别编写 Mapper，Reducer，Driver。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817200102256.png" alt="image-20220817200102256"></p>
</li>
<li><p>环境准备</p>
<p> 就是创建maven工程，然后引入hadoop依赖，和HDFS的API操作中的一样。</p>
</li>
<li><p>编写程序</p>
<ol>
<li><p>Mapper</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817202237084.png" alt="image-20220817202237084"></p>
<p> 第一个是hadoop2.x和hadoop3.x需要继承的类。第二个是hadoop1.x的。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 注意Mapper中的类型都是包org.apache.hadoop.io中的包</p>
<p> 此时的import：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br></pre></td></tr></table></figure>
<p> 完整代码：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 输入：</span></span><br><span class="line"><span class="comment"> *      KEY ： LongWritable  // 第几行</span></span><br><span class="line"><span class="comment"> *      VALUE ： Text        // 该行的内容</span></span><br><span class="line"><span class="comment"> * 输出：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outKEY</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outVALUE</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写map方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 输入数据的KEY</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 输入数据的VALUE</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 充当上下文，进行map和reduce之间的联络</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行 zyr zyr zyr</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割 [zyr,zyr,zyr]</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">// 封装outKEY</span></span><br><span class="line">            outKEY.set(word);</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(outKEY,outVALUE);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer</p>
<p> 导包规则和Mapper中的一样，不要导错了。</p>
<p> 完整代码：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.mapreduce.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里和Mapper中的输入输出刚好相反</span></span><br><span class="line"><span class="comment"> * 输入：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> * 输出：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   输入的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values    输入的value，是一个可迭代对象(不是迭代器，它时一个集合，如果想要遍历它，需要先用value.iterator()得到一个迭代器，或者增强for循环，集合的遍历方式都可)</span></span><br><span class="line"><span class="comment">     *      例如zyr zyr zyr,被mapper弄成了三个，所以传入的K-V实际上是三组 zyr-1。但是Reducer在接收数据时会转化为：zyr-(1,1,1)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   上下文作用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 累加  zyr-(1,1,1)</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key,outValue);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver</p>
<p> 写Driver类有固定套路，==最需要注意的是导包（一般就是hadoop中的包，然后两个一样的选长的）==：</p>
<ol>
<li>获取配置信息以及获取 job 对象</li>
<li>关联本 Driver 程序的 jar</li>
<li>关联 Mapper 和 Reducer 的 jar</li>
<li>设置 Mapper 输出的 kv 类型</li>
<li>设置最终输出 kv 类型</li>
<li>设置输入和输出路径</li>
<li><p>提交 job</p>
<p>完整代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//        1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class); <span class="comment">// 关联 Mapper</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class); <span class="comment">// 关联 Reducer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        4. 设置 Mapper 输出的 kv 类型 ,注意有Map，没有map的是第5步调用的方法</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello_output&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);<span class="comment">// 参数true表示需要获取job的工作信息，false不用</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 根据结果返回值，如果result == true（成功）返回0，如果失败返回1</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>本地测试</p>
<ol>
<li>需要配置好HADOOP_HOME环境变量以及Windows运行依赖</li>
<li>在IDEA上运行程序（driver写成的一个main函数）</li>
</ol>
</li>
<li><p>运行结果：</p>
<ol>
<li><p>控制台输出很多数据，是因为第7步的时候的参数是true。但是如果不是false也会有很多输出，只是没有true的时候那么多</p>
</li>
<li><p>产生一个文件夹，里面有四个文件</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817211510195.png" alt="image-20220817211510195" style="zoom: 50%;" /></p>
<p> 前面两个是校验。_SUCCESS中啥也没有，第四个文件才是结果。</p>
</li>
<li><p>打开part-r-00000,查看结果</p>
<ul>
<li><p>part表示部分（块）</p>
</li>
<li><p>r表示reducer处理结果，</p>
</li>
<li><p>00000是第几块（00000，00001，—-99999），这里数据小，只有00000</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817211805514.png" alt="image-20220817211805514" style="zoom:50%;" /></p>
<p>发现结果符合预期。</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="1-9、提交到集群测试"><a href="#1-9、提交到集群测试" class="headerlink" title="1.9、提交到集群测试"></a>1.9、提交到集群测试</h2><p>注意：</p>
<p>Driver类中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello.txt&quot;</span>));</span><br><span class="line">FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello_output&quot;</span>));</span><br></pre></td></tr></table></figure>
<p>配死了文件的路径，我们需要改一下，不然集群上面没有，会报错。</p>
<p>改成：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure>
<p>这样就可以像之前的hadoop自带的WordCount一样使用了。</p>
<ol>
<li><p>用maven打jar包，需要添加的打包插件依赖</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--      下面配置表示把我们pom中的依赖一起打包进去--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将程序打成 jar 包</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817214434565.png" alt="image-20220817214434565"></p>
<p> 可以看出带依赖的打包文件的内存都比较大。（这部分不会可以去学学Maven，Java相关开发一般用得比较多）</p>
</li>
<li><p>这里没有用到多余的依赖，Linux上都有，所以用不带依赖的jar包，改名为wc.jar，并拷贝到Hadoop集群的<code>/usr/local/hadoop</code>路径下</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817215101669.png" alt="image-20220817215101669"></p>
</li>
<li><p>启动Hadoop集群，创建测试文件夹和文件</p>
<p> 先创建好输入输出的文件夹，并产生word.txt文件</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817222143787.png" alt="image-20220817222143787"></p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817222156818.png" alt="image-20220817222156818"></p>
</li>
<li><p>执行WordCount程序</p>
<p> 调用的时候要拷贝全类名，因为我们这个jar包中有两个Driver类</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar wc.jar com.zhang.mapreduce.wordcount2.WordCountDriver /input output/</span><br></pre></td></tr></table></figure>
<p> 效果：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817223626175.png" alt="image-20220817223626175"></p>
<p> yarn的任务资源调度页面也出现新的命令。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817222800251.png" alt="image-20220817222800251"></p>
<p> 在NameNode web端中：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817223653739.png" alt="image-20220817223653739"></p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220817223743268.png" alt="image-20220817223743268" style="zoom:50%;" /></p>
<p> 可以看到效果是正确的。</p>
</li>
</ol>
<h1 id="二、Hadoop序列化"><a href="#二、Hadoop序列化" class="headerlink" title="二、Hadoop序列化"></a>二、Hadoop序列化</h1><h2 id="2-1、概念"><a href="#2-1、概念" class="headerlink" title="2.1、概念"></a>2.1、概念</h2><ol>
<li><p>什么是序列化</p>
<p> 序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于（持久化）存储到磁盘和网络传输。</p>
<p> 反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>
<p> 序列化：内存=&gt;磁盘      反序列化：磁盘=&gt;内存</p>
</li>
<li><p>为什么要序列化</p>
<p> 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而<strong>序列化可以存储“活的” 对象，可以将“活的”对象发送到远程计算机</strong>。</p>
</li>
<li><p>为什么不用 Java 的序列化</p>
<p> Java 的序列化是一个<strong>重量级</strong>序列化框架（Serializable），一个对象被序列化后，会<strong>附带很多<u>额外</u>的信息</strong>（各种校验信息，Header，继承体系等），<strong>不便于</strong>在网络中<strong>高效</strong>传输。所以， Hadoop 自己开发了一套序列化机制（Writable）。</p>
</li>
<li><p>Hadoop 序列化特点：</p>
<ol>
<li>紧凑 ：高效使用存储空间。 </li>
<li>快速：读写数据的额外开销小。</li>
<li>互操作：支持多语言的交互</li>
</ol>
</li>
</ol>
<h2 id="2-2、自定义bean对象实现序列化接口（Writable）"><a href="#2-2、自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.2、自定义bean对象实现序列化接口（Writable）"></a>2.2、自定义bean对象实现序列化接口（Writable）</h2><blockquote>
<p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在 Hadoop 框架内部传递一个 bean 对象，那么该对象就需要实现序列化接口。我们就要自定义这个接口。</p>
</blockquote>
<p>具体步骤，分为七步：</p>
<ol>
<li><p>必须实现（implements）Writable接口</p>
</li>
<li><p>反序列化时，需要反射调用空参构造函数，所以必须有空参构造函数</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="built_in">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重写序列化方法</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    out.writeLong(upFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重写反序列化方法</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    upFlow = in.readLong();</span><br><span class="line">    downFlow = in.readLong();</span><br><span class="line">    sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意反序列化的顺序和序列化的顺序完全一致</p>
</li>
<li><p>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续使用。</p>
</li>
<li><p>如果需要自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReducer框架中的Shuffle过程要求对key必须能排序。（如果只在value中传输就可以不实现）</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line">    <span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="2-3、序列化案例实操"><a href="#2-3、序列化案例实操" class="headerlink" title="2.3、序列化案例实操"></a>2.3、序列化案例实操</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>统计每一个手机号耗费的总上行流量、总下行流量、总流量</p>
<ol>
<li><p>输入数据</p>
<p> phone_data.txt    为了便于切割，用的tab。注意有手机号重复的，应当一起加起来，有IP地址缺失的（上行和下行流量分开取数组倒数几个）。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818100609043.png" alt="image-20220818100609043"></p>
</li>
<li><p>输入数据格式</p>
 <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  7 	13560436666 	120.196.100.99 	1116	954		200</span><br><span class="line"><span class="comment">%id 	手机号码      	网络ip         	上行流量 下行流量  网络状态码</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>期望输出数据格式</p>
 <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13560436666 1116    954    2070</span><br><span class="line"><span class="comment">%手机号码    上行流量 下行流量 总流量</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818093304143.png" alt="image-20220818093304143"></p>
<blockquote>
<p>总结：</p>
<p>Map阶段：</p>
<ul>
<li>输入：<ul>
<li>key：行号                    LongWritable</li>
<li>value：一行的信息     Text</li>
</ul>
</li>
<li>输出：<ul>
<li>key：手机号              Text</li>
<li>value：bean对象        FlowBean</li>
</ul>
</li>
</ul>
<p>Reduce阶段</p>
<ul>
<li><p>输入：</p>
<ul>
<li><p>key：手机号              Text</p>
</li>
<li><p>value：bean对象        FlowBean</p>
</li>
</ul>
</li>
<li><p>输出：</p>
<ul>
<li><p>key：手机号              Text</p>
</li>
<li><p>value：bean对象        FlowBean</p>
</li>
</ul>
</li>
</ul>
<p>bean对象：具有以下属性：</p>
<ul>
<li>手机号，以key的形式传播了，可有可无</li>
<li>上行流量</li>
<li>下行流量</li>
<li>总流量可以计算得到，在那里计算都一样。可有可无</li>
</ul>
</blockquote>
<h3 id="编写-MapReduce-程序"><a href="#编写-MapReduce-程序" class="headerlink" title="编写 MapReduce 程序"></a>编写 MapReduce 程序</h3><ol>
<li><p>编写流量统计的Bean对象——FlowBean</p>
<p> 还是提醒导包。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1. 实现writable接口</span></span><br><span class="line"><span class="comment"> * 2. 重写序列化和反序列化方法</span></span><br><span class="line"><span class="comment"> * 3. 重写空参构造</span></span><br><span class="line"><span class="comment"> * 4. 重写toString方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 属性</span></span><br><span class="line">    <span class="keyword">private</span> Long upFlow; <span class="comment">// 上行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long downFlow; <span class="comment">// 下行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long sumFlow; <span class="comment">// 总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空参构造</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写序列化方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataOutput</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 序列化的顺序无所谓，但是要保证序列化和反序列化的顺序一致</span></span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写反序列化方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataInput</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重写toString()方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// getter和setter</span></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(Long upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(Long downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Mapper类——FlowMapper</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1. 获取一行信息</span></span><br><span class="line">        <span class="comment">// 7 	13560436666 	120.196.100.99 	1116	954		200</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 切割</span></span><br><span class="line">        <span class="comment">// [7,13560436666,120.196.100.99,1116,954,200]</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 提取想要的数据</span></span><br><span class="line">        <span class="comment">// 手机号：13560436666=&gt;split[1],上行流量：1116=&gt;split[len - 3],下行流量：954=&gt;split[len - 2]</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">up</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">down</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 4. 封装输出Key-Value</span></span><br><span class="line">        outKey.set(phoneNum);</span><br><span class="line">        outValue.setUpFlow(Long.parseLong(up));</span><br><span class="line">        outValue.setDownFlow(Long.parseLong(down));</span><br><span class="line">        outValue.setSumFlow();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 5.写出</span></span><br><span class="line">        context.write(outKey, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Reducer类——FlowReducer</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 遍历 values,将其中的上行流量,下行流量分别累加</span></span><br><span class="line">        <span class="comment">// phoneNum - [bean1,bean2]</span></span><br><span class="line">        <span class="type">Long</span> <span class="variable">sumUp</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">        <span class="type">Long</span> <span class="variable">sumDown</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            sumUp += value.getUpFlow();</span><br><span class="line">            sumDown += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 封装 outKV</span></span><br><span class="line">        outValue.setUpFlow(sumUp);</span><br><span class="line">        outValue.setDownFlow(sumDown);</span><br><span class="line">        outValue.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 写出</span></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Driver驱动类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">//1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置 Mapper 输出的 kv 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\phone_data .txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试结果</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818104317585.png" alt="image-20220818104317585" style="zoom:50%;" /></p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818104114200.png" alt="image-20220818104114200" style="zoom:50%;" /></p>
<p> 结果正确。</p>
</li>
</ol>
<h1 id="三、MapReduce-框架原理"><a href="#三、MapReduce-框架原理" class="headerlink" title="三、MapReduce 框架原理"></a>三、MapReduce 框架原理</h1><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818104657592.png" alt="image-20220818104657592"></p>
<h2 id="3-1、InputFormat-数据输入"><a href="#3-1、InputFormat-数据输入" class="headerlink" title="3.1、InputFormat 数据输入"></a>3.1、InputFormat 数据输入</h2><h3 id="3-1-1-切片于MapTask并行度决定机制"><a href="#3-1-1-切片于MapTask并行度决定机制" class="headerlink" title="3.1.1 切片于MapTask并行度决定机制"></a>3.1.1 切片于MapTask并行度决定机制</h3><ol>
<li><p>问题引出</p>
<blockquote>
<p>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</p>
</blockquote>
<p> 思考：1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数 据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因素影响了 MapTask 并行度？</p>
</li>
<li><p>MapTask 并行度决定机制</p>
<p> 数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>
<p> 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818105208230.png" alt="image-20220818105208230"></p>
</li>
</ol>
<p><a href="#3.5.3、ReduceTask并行度决定机制">ReduceTask并行度决定机制</a></p>
<h3 id="3-1-2、Job提交流程源码详解"><a href="#3-1-2、Job提交流程源码详解" class="headerlink" title="3.1.2、Job提交流程源码详解"></a>3.1.2、Job提交流程源码详解</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自己代码中调用的函数</span></span><br><span class="line">job.waitForCompletion()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 进入函数第一个方法</span></span><br><span class="line">    submit();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// submit()中第一个函数（ensureState）是判断是否有异常</span></span><br><span class="line">        <span class="comment">// 第二个函数（setUseNewAPI）是处理版本兼容用的</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 真正关心的函数：</span></span><br><span class="line">        <span class="comment">// 1 建立连接</span></span><br><span class="line">        connect();</span><br><span class="line">            <span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line">            <span class="type">Cluster</span> <span class="variable">cluster</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line">                <span class="comment">// 在Cluster类的构造方法中判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">                initialize(jobTrackAddr, conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 提交 job</span></span><br><span class="line">        submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br><span class="line">            <span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line">            <span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">            <span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line">            <span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line">            <span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">            copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">            rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">            <span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">            writeSplits(job, submitJobDir);</span><br><span class="line">            maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">            input.getSplits(job);</span><br><span class="line">            <span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">            writeConf(conf, submitJobFile);</span><br><span class="line">            conf.writeXml(out);</span><br><span class="line">            <span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">            status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818192246053.png" alt="image-20220818192246053"></p>
<h3 id="3-1-3、-FileInputFormat-切片源码和切片机制"><a href="#3-1-3、-FileInputFormat-切片源码和切片机制" class="headerlink" title="3.1.3、 FileInputFormat 切片源码和切片机制"></a>3.1.3、 FileInputFormat 切片源码和切片机制</h3><ol>
<li><p>FileInputFormat 切片源码解析（input.getSplits(job))</p>
<ol>
<li><p>程序先找到你数据存储的目录。</p>
</li>
<li><p>开始遍历处理（规划切片）目录下的每一个文件</p>
</li>
<li>遍历第一个文件ss.txt<ol>
<li>获取文件大小fs.sizeOf(ss.txt)</li>
<li>计算切片大小 computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M</li>
<li><strong>默认情况下，切片大小=blocksize</strong></li>
<li>开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M （<strong>每次切片时，都要判断切完剩下的部分是否大于块的==1.1倍==，不大于1.1倍就划分一块切片</strong>）</li>
<li>将切片信息写到一个切片规划文件中 f）整个切片的核心过程在getSplit()方法中完成</li>
<li><strong>InputSplit只记录了切片的元数据信息</strong>，比如起始位置、长度以及所在的节点列表等。</li>
</ol>
</li>
<li><strong>提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数</strong></li>
</ol>
</li>
<li><p>FileInputFormat 切片机制</p>
<ol>
<li><p>切片机制</p>
<ol>
<li>简单地按照文件的内容长度进行切片</li>
<li>切片大小，默认等于Block大小</li>
<li><strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</strong></li>
</ol>
</li>
<li><p>案例分析：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818194643496.png" alt="image-20220818194643496" style="zoom:50%;" /></p>
<ol>
<li><p>源码中计算切片大小的公式：</p>
<ul>
<li><code>Math.max(minSize, Math.min(maxSize, blockSize))</code></li>
<li><code>mapreduce.input,fileformat.split.minsize=1</code>      <strong>默认值为1</strong></li>
<li><p><code>mapreduce.input.fileinputformat.split.maxsize=Long.MAX_VALUE</code>   <strong>默认值时Long的最大值</strong></p>
<p>==所以默认情况下，切片大小=blocksize==</p>
</li>
</ul>
</li>
<li><p>切片大小设置</p>
<ul>
<li><code>maxsize</code>（切片最大值）：参数如果比blockSize小，则会让切片变小，而且就等于配置的这个参数的值</li>
<li><code>minsize</code>（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大</li>
</ul>
</li>
<li><p>获取切片信息的API</p>
<ul>
<li><p>获取切片的文件名称：</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> inputSplit.getPath().getName();</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据文件类型获取切片信息</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">FileSplit</span> <span class="variable">inputSplit</span> <span class="operator">=</span> (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="3-1-4、TextInputFormat"><a href="#3-1-4、TextInputFormat" class="headerlink" title="3.1.4、TextInputFormat"></a>3.1.4、TextInputFormat</h3><blockquote>
<p>思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</p>
<p>FileInputFormat 常见的接口实现类包括：<strong>TextInputFormat</strong>、KeyValueTextInputFormat、 NLineInputFormat、<strong>CombineTextInputFormat</strong> 和自定义 InputFormat 等。</p>
</blockquote>
<p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。</p>
<p>以下是一个示例，比如，一个分片包含了如下 4 条文本记录</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br></pre></td></tr></table></figure>
<p>每条记录表示为以下键/值对：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(0,Rich learning form)</span><br><span class="line">(20,Intelligent learning engine)</span><br><span class="line">(49,Learning more convenient)</span><br><span class="line">(74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-5、CombineTextInputFormat-切片机制"><a href="#3-1-5、CombineTextInputFormat-切片机制" class="headerlink" title="3.1.5、CombineTextInputFormat 切片机制"></a>3.1.5、CombineTextInputFormat 切片机制</h3><blockquote>
<p>框架默认的 <strong>TextInputFormat</strong> 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有<strong>大量小文件</strong>，就会产生大量的 MapTask，<strong>处理效率极其低下。</strong></p>
</blockquote>
<p>==CombineTextInputFormat 切片机制就是解决大量小文件的办法之一==</p>
<ol>
<li><p>应用场景</p>
<p> <strong>CombineTextInputFormat 用于小文件过多的场景</strong>，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
</li>
<li><p>虚拟存储切片最大值设置</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);<span class="comment">// 4m </span></span><br></pre></td></tr></table></figure>
<p> 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
</li>
<li><p>切片机制</p>
<p> 生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<ul>
<li><p>虚拟存储过程：</p>
<p>  将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较</p>
<ul>
<li>如果不大于设置的最大值，逻辑上划分一个块。</li>
<li>如果输入文件大于设置的最大值且大于两倍， 那么以最大值切割一块；</li>
<li><p><strong>当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</strong></p>
<p>例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储 文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</p>
</li>
</ul>
</li>
<li><p>切片过程：</p>
<ul>
<li>判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值<ul>
<li>大于等于则单独形成一个切片。</li>
<li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>测试举例：</p>
<p> 有 4 个小文件大小分别为 1.7M、5.1M、3.4M 以及 6.8M 这四个小文件。</p>
<p> 则虚拟存储之后形成 6 个文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M 以及（3.4M、3.4M）</p>
<p> 最终会形成 3 个切片，大小分别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818204040423.png" alt="image-20220818204040423"></p>
<h2 id="3-2、MapReduce-工作流程"><a href="#3-2、MapReduce-工作流程" class="headerlink" title="3.2、MapReduce 工作流程"></a>3.2、MapReduce 工作流程</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818204953965.png" alt="image-20220818204953965"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818205006031.png" alt="image-20220818205006031"></p>
<p>上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第 16 步结束，具体 Shuffle 过程详解，如下： </p>
<ol>
<li>MapTask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li>
<li>多个溢出文件会被合并成大的溢出文件</li>
<li>在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</li>
<li>ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</li>
<li>ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序） </li>
<li>合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过 程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法） </li>
</ol>
<p>注意：</p>
<ol>
<li>Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快。</li>
<li>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</li>
</ol>
<blockquote>
<p>为什么排序，排序排的是什么？</p>
<p>我的思考：排序是拍的输出数据的key的顺序。我们之前做的WordCount的案例的结果就是按照单词（也就是输出数据的key）的字典序进行排序的。所以这里的快排和归并排序就是排输出数据的key的顺序</p>
</blockquote>
<h2 id="3-3、Shuffle-机制"><a href="#3-3、Shuffle-机制" class="headerlink" title="3.3、Shuffle 机制"></a>3.3、Shuffle 机制</h2><h3 id="3-3-1、Shuffle机制"><a href="#3-3-1、Shuffle机制" class="headerlink" title="3.3.1、Shuffle机制"></a>3.3.1、Shuffle机制</h3><blockquote>
<p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle（洗牌）。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818210524303.png" alt="image-20220818210524303"></p>
<h3 id="3-3-2、Partition-分区"><a href="#3-3-2、Partition-分区" class="headerlink" title="3.3.2、Partition 分区"></a>3.3.2、Partition 分区</h3><ol>
<li><p>需求</p>
<p> 要求将统计结果按照条件输出到不同文件中（分区）。</p>
<p> 比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
</li>
<li><p>默认 Partitioner 分区</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line">    	<span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p>
</li>
<li><p>自定义 Partitioner 步骤</p>
<ol>
<li><p>自定义继承Partitioner的类，重写getPartition()方法</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">        … …</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Job驱动中，设置自定义Patitioner</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPatitioner.class);</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义Patition后，要根据自定义Patitioner的逻辑设置相应数量的ReduceTask</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>分区总结</p>
<ol>
<li>如果<code>ReduceTask的数量 &gt; getPartition的结果数</code>，则会多产生几个空的输出文件part-r-000xx；</li>
<li>如果<code>1 &lt; ReduceTask的数量 &lt; getPartition的结果数</code>，则有一部分分区数据无处安放，会Exception；</li>
<li>如果<code>ReduceTask的数量 = 1</code>，则不管MapTask端输出多少个分区文件，最终结果都交给这一个<br> ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</li>
<li><p>分区号必须从零开始，逐一累加</p>
<p>例如：假设自定义分区数为5，则</p>
</li>
<li><p>job.setNumReduceTasks(1); 小于5，会正常运行，只不过会产生一个输出文件</p>
</li>
<li>job.setNumReduceTasks(2); 大于1，小于5，会报错</li>
<li>job.setNumReduceTasks(6); 大于5，程序会正常运行，会产生空文件</li>
</ol>
</li>
</ol>
<h3 id="3-3-3、-Partition-分区案例实操"><a href="#3-3-3、-Partition-分区案例实操" class="headerlink" title="3.3.3、 Partition 分区案例实操"></a>3.3.3、 Partition 分区案例实操</h3><ol>
<li><p>需求</p>
<p> 将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<ul>
<li><p>输入数据</p>
<p>  phone_data.txt</p>
</li>
<li><p>期望输出数据</p>
<p>  手机号136、137、138、139开头都分别放到一个独立的文件中，其他开头通通放到一个文件中（总共5个文件）</p>
</li>
</ul>
</li>
<li><p>需求分析</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220818214147665.png" alt="image-20220818214147665"></p>
</li>
<li><p>在<a href="#2.3、序列化案例实操">案例2.3</a>的基础上修改</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text,FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">// text 是手机号</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 获取前三位</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">prePhoneNum</span> <span class="operator">=</span> phoneNum.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置分区</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="string">&quot;136&quot;</span>.equals(prePhoneNum))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;137&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;138&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;139&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 再修改一下Driver类，添加8，9两步（在第5步后面）。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">//1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置 Mapper 输出的 kv 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//8. 指定自定义分区器</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//9. 同时指定相应数量的 ReduceTask</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\phone_data .txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 注意：NumReduceTasks的数量最好和分区数相同，效果最好。但是也可以是1（相当于白分区了（不设置的默认值也是1））或比5大（会多产生空文件）。但是大于1，小于5就会报错。原因看上一节。</p>
</li>
<li><p>结果</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819093104178.png" alt="image-20220819093104178" style="zoom:50%;" /></p>
<p> 确实产生了五个分区文件。</p>
<p> 内容：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819093340612.png" alt="image-20220819093340612" style="zoom:50%;" /></p>
</li>
</ol>
<h3 id="3-3-4、WritableComparable-排序"><a href="#3-3-4、WritableComparable-排序" class="headerlink" title="3.3.4、WritableComparable 排序"></a>3.3.4、WritableComparable 排序</h3><blockquote>
<p>排序是MapReduce框架中最重要的操作之一。</p>
<p>MapTask和ReduceTask均会对数据按照<strong>key</strong>进行排序。该操作属于Hadoop的<strong>默认行为</strong>。任何应用程序中的数据均会被排序，而<strong>不管逻辑上是否需要</strong>。</p>
<p><strong>默认</strong>排序是<strong>按照字典顺序排序</strong>，且实现该排序的方法是<strong>快速排序</strong>。</p>
<p>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率<strong>达到一定阈值后</strong>，再对<strong>缓冲区中的数据</strong>进行一次<strong>快速排序</strong>，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上<strong>所有文件</strong>进行<strong>归并排序</strong>。</p>
<p>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件</p>
<ul>
<li>如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。</li>
<li>如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；</li>
<li>如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。</li>
<li>当所有数据拷贝完毕后，<strong>ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</strong></li>
</ul>
</blockquote>
<ol>
<li><p>排序分类</p>
<ol>
<li><p>部分排序</p>
<p> MapReduce根据输入记录的键对数据集排序。保证输出的每个文件<strong>内部有序</strong>。</p>
</li>
<li><p>全排序</p>
<p> <strong>最终输出结果只有一个文件，且文件内部有序</strong>。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</p>
</li>
<li><p>辅助排序：（GroupingComparator 分组）</p>
<p> 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p>
</li>
<li><p>二次排序</p>
<p> 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p>
</li>
</ol>
</li>
<li><p>自定义排序 WritableComparable 原理分析</p>
<p> bean 对象做为 key 传输，需要实现 <code>WritableComparable</code> 接口重写 <code>compareTo</code> 方法，就可以实现排序。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean bean)</span> &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> result;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">    	result = -<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">    	result = <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">    	result = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-3-5、WritableComparable-排序案例实操（全排序）"><a href="#3-3-5、WritableComparable-排序案例实操（全排序）" class="headerlink" title="3.3.5、WritableComparable 排序案例实操（全排序）"></a>3.3.5、WritableComparable 排序案例实操（全排序）</h3><ol>
<li><p>需求</p>
<p> 根据<a href="#2.3、序列化案例实操">案例2.3</a>序列化案例<strong>产生的结果</strong>，再次对<strong>总流量</strong>进行<strong>倒序</strong>排序，如果总流量一样，就<strong>上行流量升序</strong>排序。</p>
</li>
<li><p>需求分析</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819095224931.png" alt="image-20220819095224931"></p>
<blockquote>
<p>Map阶段：</p>
<p>输入：</p>
<ul>
<li>key：LongWritable  行号</li>
<li>value：Text            一行数据</li>
</ul>
<p>输出：</p>
<ul>
<li>key：FlowBean       自定义Bean（要求可排序）</li>
<li>value：Text             手机号</li>
</ul>
<p>Reduce阶段：</p>
<p>输入：</p>
<ul>
<li>key：FlowBean         自定义Bean</li>
<li>value：Text             手机号</li>
</ul>
<p>输出：</p>
<ul>
<li>key：Text                    手机号</li>
<li>value：FlowBean       自定义Bean</li>
</ul>
</blockquote>
</li>
<li><p>代码实现</p>
<ol>
<li><p>FlowBean 对象在在需求 1 基础上<strong>增加了比较功能</strong></p>
<ul>
<li>原本实现Writable接口，现在修改为实现WritableComparable接口（注意，这是一个接口，之前实现成了Writable和Comparable两个接口，就无法比较会报错），泛型是FlowBean（同类比较）</li>
<li><p>实现ompareTo方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1. 实现writable接口</span></span><br><span class="line"><span class="comment"> * 2. 重写序列化和反序列化方法</span></span><br><span class="line"><span class="comment"> * 3. 重写空参构造</span></span><br><span class="line"><span class="comment"> * 4. 重写toString方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Long upFlow; <span class="comment">// 上行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long downFlow; <span class="comment">// 下行流量</span></span><br><span class="line">    <span class="keyword">private</span> Long sumFlow; <span class="comment">// 总流量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空参构造</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写序列化方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataOutput</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// 序列化的顺序无所谓，但是要保证序列化和反序列化的顺序一致</span></span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写反序列化方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataInput</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重写toString()方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line">        <span class="comment">// 总流量的倒序排序</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">this</span>.sumFlow &gt; o.sumFlow)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 按上行流量正序排序</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">this</span>.upFlow &gt; o.upFlow)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( <span class="built_in">this</span>.upFlow &lt; o.upFlow)&#123;</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getUpFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpFlow</span><span class="params">(Long upFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getDownFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownFlow</span><span class="params">(Long downFlow)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>编写Mapper类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">FlowBean</span> <span class="variable">outKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean, Text&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取一行 13315688577	4481	22681	27162</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="comment">// 切割 [13315688577,4481,22681,27162]</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 封装</span></span><br><span class="line">        outKey.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outKey.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outKey.setSumFlow();</span><br><span class="line">        outValue.set(split[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(outKey, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Reduce类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写Driver类</p>
<ul>
<li>修改Mapper输出的key和value</li>
<li><p>修改输入输出的路径（输入数据是之前的结果，要把其他三个文件删掉，否则会一起存）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">//1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置 Mapper 输出的 kv 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output2&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>效果：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819150121474.png" alt="image-20220819150121474" style="zoom:50%;" /></p>
<p> 结果的确倒序排序了。</p>
</li>
</ol>
</li>
</ol>
<h3 id="3-3-6、WritableComparable排序案例实操（区内排序）"><a href="#3-3-6、WritableComparable排序案例实操（区内排序）" class="headerlink" title="3.3.6、WritableComparable排序案例实操（区内排序）"></a>3.3.6、WritableComparable排序案例实操（区内排序）</h3><ol>
<li><p>需求</p>
<p> 要求每个省份手机号输出的文件中按照总流量内部排序。</p>
</li>
<li><p>需求分析</p>
<p> 基于前一个需求，增加自定义分区类，<a href="#3.3.3、 Partition 分区案例实操">分区</a>按照省份手机号设置</p>
</li>
<li><p>案例实操</p>
<ol>
<li><p>添加自定义分区类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text,FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">        <span class="comment">// text 是手机号</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 获取前三位</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">prePhoneNum</span> <span class="operator">=</span> phoneNum.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置分区</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="string">&quot;136&quot;</span>.equals(prePhoneNum))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;137&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;138&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;139&quot;</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在驱动类中添加分区</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//8. 指定自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//9. 同时指定相应数量的 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>效果</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819151333412.png" alt="image-20220819151333412"></p>
</li>
</ol>
<h3 id="3-3-7、Combiner-合并"><a href="#3-3-7、Combiner-合并" class="headerlink" title="3.3.7、Combiner 合并"></a>3.3.7、Combiner 合并</h3><blockquote>
<p>Combiner是MR程序中Mapper和Reducer之外的一种组件。</p>
<p>Combiner组件的父类就是Reducer。</p>
<p>Combiner和Reducer的区别在于运行的位置</p>
<ul>
<li>Combiner是在每一个MapTask所在的节点运行;</li>
<li>Reducer是接收全局所有Mapper的输出结果；</li>
</ul>
<p>Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</p>
<p>Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</p>
</blockquote>
<p>自定义Combiner 实现步骤</p>
<ol>
<li><p>自定义一个Combiner继承Reducer，重写Reduce方法</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Contextcontext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        context.write(key,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Job驱动类中设置：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-3-8、Combiner合并案例实操"><a href="#3-3-8、Combiner合并案例实操" class="headerlink" title="3.3.8、Combiner合并案例实操"></a>3.3.8、Combiner合并案例实操</h3><ol>
<li><p>需求</p>
<p> 统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</p>
</li>
<li><p>需求分析</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819152417658.png" alt="image-20220819152417658"></p>
</li>
<li><p>案例实操——方案一</p>
<ol>
<li><p>增加一个WordCountCombiner类继承Reducer</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable,Text, IntWritable&gt; &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在WordCountDriver驱动类中指定Combiner</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>案例实操——方案二</p>
<ol>
<li><p>将WordCountReducer 作为 Combiner 在 WordCountDriver驱动类中指定</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h2 id="3-4、OutPutFormat-数据输出"><a href="#3-4、OutPutFormat-数据输出" class="headerlink" title="3.4、OutPutFormat 数据输出"></a>3.4、OutPutFormat 数据输出</h2><blockquote>
<p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat 接口。下面我们介绍几种常见的OutputFormat实现类。</p>
</blockquote>
<h3 id="3-4-1、OutputFormat-接口实现类"><a href="#3-4-1、OutputFormat-接口实现类" class="headerlink" title="3.4.1、OutputFormat 接口实现类"></a>3.4.1、OutputFormat 接口实现类</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819154538228.png" alt="image-20220819154538228"></p>
<p>默认输出格式<code>TextOutputFormat</code></p>
<p>自定义OutputFormat：</p>
<ol>
<li><p>应用场景：</p>
<p> 输出数据到MySQL/HBase/Elasticsearch等存储框架中。</p>
</li>
<li><p>自定义OutputFormat步骤</p>
<ol>
<li>自定义一个类继承<code>FileOutPutFormat</code></li>
<li>改写<code>RecordWriter</code>，具体改写输出数据的方法<code>write()</code></li>
</ol>
</li>
</ol>
<h3 id="3-4-2、自定义OutputFormat案例实操"><a href="#3-4-2、自定义OutputFormat案例实操" class="headerlink" title="3.4.2、自定义OutputFormat案例实操"></a>3.4.2、自定义OutputFormat案例实操</h3><ol>
<li><p>需求</p>
<p> 过滤输入的 log 日志，包含 study 的网站输出到 e:/study.log，不包含 study 的网站输出到 e:/other.log。</p>
</li>
<li><p>需求分析</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819154957495.png" alt="image-20220819154957495"></p>
</li>
<li><p>案例实操</p>
<ol>
<li><p>编写 LogMapper 类（Map阶段不做任何操作）</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 在Map阶段不做任何处理</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 LogReducer 类（Reduce阶段基本上也是不做任何操作）</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 防止有相同数据，丢数据</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义一个 LogOutputFormat 类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title class_">FileOutputFormat</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title function_">getRecordWriter</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">LogRecordWriter</span> <span class="variable">logRecordWriter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogRecordWriter</span>(taskAttemptContext);</span><br><span class="line">        <span class="keyword">return</span> logRecordWriter;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 LogRecordWriter 类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title class_">RecordWriter</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream study;</span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream other;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构造方法，参数是job，要利用job的配置对象创建文件系统</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> &#123;</span><br><span class="line">        <span class="comment">// 创建两个流，一个流存喊study的数据，一个流存其他的不含study的数据</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(job.getConfiguration());</span><br><span class="line">            study = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\study.log&quot;</span>));</span><br><span class="line">            other = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\other.log&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Text text, NullWritable nullWritable)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">log</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 具体写</span></span><br><span class="line">        <span class="keyword">if</span> (log.contains(<span class="string">&quot;study&quot;</span>))&#123;</span><br><span class="line">            study.writeBytes(log + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            other.writeBytes(log + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 关流</span></span><br><span class="line">        IOUtils.close(study);</span><br><span class="line">        IOUtils.close(other);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 LogDriver 类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自定义的 outputFormat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\log.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// 虽然我们自定义了outputFormat，但是因为我们的outputFormat继承自fileOutputFormat而fileOutputFormat要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>效果</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819162955910.png" alt="image-20220819162955910" style="zoom:50%;" /></p>
<p> 第二个指定FileOutputFormat实际上就是输出一个标记而已，指定的文件夹中只有success。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819163257465.png" alt="image-20220819163257465" style="zoom:50%;" /></p>
<p> 输出结果也满足预期。</p>
</li>
</ol>
</li>
</ol>
<h2 id="3-5、MapReduce-内核源码解析"><a href="#3-5、MapReduce-内核源码解析" class="headerlink" title="3.5、MapReduce 内核源码解析"></a>3.5、MapReduce 内核源码解析</h2><h3 id="3-5-1、MapTask工作机制"><a href="#3-5-1、MapTask工作机制" class="headerlink" title="3.5.1、MapTask工作机制"></a>3.5.1、MapTask工作机制</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819163406982.png" alt="image-20220819163406982"></p>
<ol>
<li>Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中 解析出一个个 key/value。</li>
<li>Map 阶段：该节点主要是将解析出的 key/value 交给用户编写 map()函数处理，并 产生一系列新的 key/value。</li>
<li>Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用 OutputCollector.collect()输出结果。在该函数内部，它会将生成的 key/value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</li>
<li>Spill 阶段（溢写阶段）：当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上， 生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。<ol>
<li>步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在 一起，且同一分区内所有数据按照 key 有序。</li>
<li>步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文 件 output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之 前，对每个分区中的数据进行一次聚集操作。</li>
<li>步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元 信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大 小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。</li>
</ol>
</li>
<li>Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并， 以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。 在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多 轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文 件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</li>
</ol>
<h3 id="3-5-2、ReduceTask-工作机制"><a href="#3-5-2、ReduceTask-工作机制" class="headerlink" title="3.5.2、ReduceTask 工作机制"></a>3.5.2、ReduceTask 工作机制</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819163938386.png" alt="image-20220819163938386"></p>
<ol>
<li>Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数 据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</li>
<li>Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁 盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用 户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一 起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了 局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</li>
<li>Reduce 阶段：reduce()函数将计算结果写到 HDFS 上。</li>
</ol>
<h3 id="3-5-3、ReduceTask并行度决定机制"><a href="#3-5-3、ReduceTask并行度决定机制" class="headerlink" title="3.5.3、ReduceTask并行度决定机制"></a>3.5.3、ReduceTask并行度决定机制</h3><p>回顾：<a href="#3.1.1 切片于MapTask并行度决定机制">MapTask并行度</a>由切片个数决定，切片个数由输入文件和切片规则决定。</p>
<p>思考：ReduceTask并行度由谁决定？</p>
<ol>
<li><p>设置ReduceTask并行度（个数）</p>
<p> ReduceTask的并行度同样影响真个Job的执行并发度和执行效率，但于MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>实验：测试ReduceTask多少合适</p>
<ol>
<li><p>实验环境：1个Master节点，16个Slave节点：CPU：8GHZ，内存：2G</p>
</li>
<li><p>实验结论：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819165406820.png" alt="image-20220819165406820" style="zoom:67%;" /></p>
</li>
<li><p>注意事项</p>
<ol>
<li><p>ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。</p>
</li>
<li><p>ReduceTask默认值就是1，所以输出文件个数为一个。</p>
</li>
<li><p>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜</p>
</li>
<li><p>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。</p>
</li>
<li><p>具体多少个ReduceTask，需要根据集群性能而定。</p>
</li>
<li><p>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。</p>
<p> 答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="3-6、Join应用"><a href="#3-6、Join应用" class="headerlink" title="3.6、Join应用"></a>3.6、Join应用</h2><h3 id="3-6-1、Reduce-Join"><a href="#3-6-1、Reduce-Join" class="headerlink" title="3.6.1、Reduce Join"></a>3.6.1、Reduce Join</h3><p>Map 端的主要工作：为来自不同表或文件的 key/value 对，<strong>打标签以区别不同来源的记录</strong>。然后<strong>用连接字段作为 key</strong>，其余部分和新加的标志作为 value，最后进行输出。</p>
<p>Reduce 端的主要工作：在 Reduce 端<strong>以连接字段作为 key 的分组已经完成</strong>，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了。</p>
<h3 id="3-6-2、Reduce-Join-案例实操"><a href="#3-6-2、Reduce-Join-案例实操" class="headerlink" title="3.6.2、Reduce Join 案例实操"></a>3.6.2、Reduce Join 案例实操</h3><ol>
<li><p>需求</p>
<p> 将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
</li>
<li><p>数据：</p>
<p> order.txt</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819173039166.png" alt="image-20220819173039166" style="zoom:50%;" /></p>
<p> pd.txt</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819173158477.png" alt="image-20220819173158477" style="zoom: 79%;" /></p>
</li>
<li><p>需求分析</p>
<p> 通过将关联条件（pid）作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220819173326552.png" alt="image-20220819173326552"></p>
<blockquote>
<ul>
<li>Map阶段：<ul>
<li>输入：<ul>
<li>key：LongWritable    行号</li>
<li>value：Text              每一行的内容</li>
</ul>
</li>
<li>输出：<ul>
<li>key：Text                 id</li>
<li>value：TableBean      自定义Bean</li>
</ul>
</li>
</ul>
</li>
<li>Reduce阶段：<ul>
<li>输入：<ul>
<li>key：</li>
<li>value：</li>
</ul>
</li>
<li>输出：<ul>
<li>key：</li>
<li>value：</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
</li>
<li><p>代码实现</p>
<ol>
<li><p>创建商品和订单合并后的TableBean类</p>
<p> 用Lombok创建的无参构造和getter和setter。</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lombok.Data;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// id pid amount</span></span><br><span class="line">    <span class="comment">// pid name</span></span><br><span class="line">    <span class="keyword">private</span> String id; <span class="comment">// 订单id</span></span><br><span class="line">    <span class="keyword">private</span> String pid; <span class="comment">// 商品id</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> amount; <span class="comment">// 商品数量</span></span><br><span class="line">    <span class="keyword">private</span> String name; <span class="comment">// 商品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag; <span class="comment">// 标记是那个表</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(name);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.pid = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.amount = dataInput.readInt();</span><br><span class="line">        <span class="built_in">this</span>.name = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// id name amount</span></span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">&#x27;\t&#x27;</span> + name + <span class="string">&#x27;\t&#x27;</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableMapper类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, TableBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileName;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">TableBean</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 初始化 order.txt  pd.txt</span></span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">split</span> <span class="operator">=</span> (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">        fileName = split.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1. 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="comment">// 2. 判断是哪一个文件的</span></span><br><span class="line">        <span class="keyword">if</span> (fileName.contains(<span class="string">&quot;order&quot;</span>)) &#123; <span class="comment">// 处理的是 order.txt</span></span><br><span class="line">            <span class="comment">// 3. 分割 [1001,01,1]</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="comment">// 4. 封装kv</span></span><br><span class="line">            outKey.set(split[<span class="number">1</span>]);</span><br><span class="line">            outValue.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outValue.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outValue.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outValue.setName(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outValue.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// 处理的是 pd.txt</span></span><br><span class="line">            <span class="comment">// 3. 分割 [01,小米]</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="comment">// 4. 封装</span></span><br><span class="line">            outKey.set(split[<span class="number">0</span>]);</span><br><span class="line">            outValue.setId(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outValue.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outValue.setAmount(<span class="number">0</span>);</span><br><span class="line">            outValue.setName(split[<span class="number">1</span>]);</span><br><span class="line">            outValue.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 5.写出</span></span><br><span class="line">        context.write(outKey, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableReducer类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Reducer&lt;Text, TableBean, TableBean, NullWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 01   1001    1   order</span></span><br><span class="line">        <span class="comment">// 01   1004    4   order</span></span><br><span class="line">        <span class="comment">// 01   小米         pd</span></span><br><span class="line">        <span class="comment">// 准备初始化集合</span></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">TableBean</span> <span class="variable">pdBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 循环遍历</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="string">&quot;order&quot;</span>.equals(value.getFlag()))&#123; <span class="comment">// 处理订单表</span></span><br><span class="line">                <span class="type">TableBean</span> <span class="variable">tempTableBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tempTableBean, value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                orderBeans.add(tempTableBean);</span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">// 处理商品表</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean, value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 循环遍历 orderBeans ,赋值 name</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            orderBean.setName(pdBean.getName());</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(orderBean, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写TableDriver类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>总结</p>
<p> 缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。 </p>
<p> ==解决方案：Map 端实现数据合并（利用Map Join）。==</p>
</li>
</ol>
<h3 id="3-6-3、Map-Join"><a href="#3-6-3、Map-Join" class="headerlink" title="3.6.3、Map Join"></a>3.6.3、Map Join</h3><ol>
<li><p>使用场景</p>
<p> Map Join适用于一张表十分小、一张表很大的场景。</p>
</li>
<li><p>优点</p>
<p> 思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？</p>
<p> 在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。</p>
</li>
<li><p>具体办法：采用DistributedCache</p>
<ol>
<li><p>在Mapper的setup阶段，将文件读取到缓存集合中</p>
</li>
<li><p>在Driver驱动类中加载缓存</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///e:/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop102:8020/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h3 id="3-6-4、Map-Join-案例实操"><a href="#3-6-4、Map-Join-案例实操" class="headerlink" title="3.6.4、Map Join 案例实操"></a>3.6.4、Map Join 案例实操</h3><ol>
<li><p>需求</p>
<p> 和<a href="#3.6.2、Reduce Join 案例实操">3.6.2、Reduce Join 案例实操</a>一样</p>
</li>
<li><p>需求分析</p>
<p> MapJoin适用于关联表中有表的情况</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820091952393.png" alt="image-20220820091952393" style="zoom:67%;" /></p>
</li>
<li><p>实现代码</p>
<ol>
<li><p>在MapJoinDriver驱动类中添加缓存文件</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MapJoinDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">// 1 获取 job 信息</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        <span class="comment">// 2 设置加载 jar 包路径</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver.class);</span><br><span class="line">        <span class="comment">// 3 关联 mapper</span></span><br><span class="line">        job.setMapperClass(MapJoinMapper.class);</span><br><span class="line">        <span class="comment">// 4 设置 Map 输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">// 5 设置最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">// 加载缓存数据</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///D:/桌面/input/pd.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 6 设置输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\input\\order.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\output&quot;</span>));</span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在MapJoinMapper类中的setup方法中读取缓存文件</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    HashMap&lt;String, String&gt; pdMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取缓存文件，并把缓存文件内容封装到集合 pd.txt中</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取流</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(cacheFiles[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从流中读取数据</span></span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(fis, <span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line">            <span class="comment">// 切割 [01,小米]</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>],split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 处理 order.txt</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="comment">// 分割 [1001,01,1]</span></span><br><span class="line">        String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 获取pid</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">pName</span> <span class="operator">=</span> pdMap.get(fields[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        outKey.set(fields[<span class="number">0</span>] + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + fields[<span class="number">2</span>]);</span><br><span class="line">        context.write(outKey, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>效果</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820101110988.png" alt="image-20220820101110988" style="zoom:50%;" /></p>
</li>
</ol>
<h2 id="3-7、数据清洗（ETL）"><a href="#3-7、数据清洗（ETL）" class="headerlink" title="3.7、数据清洗（ETL）"></a>3.7、数据清洗（ETL）</h2><p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取 （Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户 要求的数据。<strong>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</strong></p>
<ol>
<li><p>需求</p>
<p> 取出日志中字段个数小于等于11的日志</p>
</li>
<li><p>需求分析</p>
<p> 需要在Map阶段对输入的数据根据规则进行过滤</p>
</li>
<li><p>实现代码</p>
<ol>
<li><p>编写WebLogMapper类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1. 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="comment">// 3. ETL</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> parseLog(line,context);</span><br><span class="line">        <span class="keyword">if</span> (!result) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 4. 写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">parseLog</span><span class="params">(String line, Context context)</span> &#123;</span><br><span class="line">        <span class="comment">// 2. 切割</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="comment">// 判断日志长度是否大于11</span></span><br><span class="line">        <span class="keyword">if</span>(split.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写WebLogDriver类</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.zhang.mapreduce.outputFormat.LogDriver;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[] &#123; <span class="string">&quot;D:/桌面/input&quot;</span>, <span class="string">&quot;D:/桌面/output&quot;</span> &#125;;</span><br><span class="line">		<span class="comment">// 1 获取 job 信息</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">		<span class="comment">// 2 加载 jar 包</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">		<span class="comment">// 3 关联 map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line">		<span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">		<span class="comment">// 设置 reduceTask 个数为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">		<span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">		<span class="comment">// 6 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h2 id="3-8、MapReduce开发总结"><a href="#3-8、MapReduce开发总结" class="headerlink" title="3.8、MapReduce开发总结"></a>3.8、MapReduce开发总结</h2><ol>
<li><p>输入数据接口：<code>InputFormat</code></p>
<ol>
<li>默认使用的实现类是：<code>TextInputFormat</code></li>
<li><code>TextInputFormat</code> 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li>
<li><code>CombineTextInputFormat</code> 可以把多个小文件合并成一个切片处理，提高处理效率。</li>
</ol>
</li>
<li><p>逻辑处理接口：Mapper</p>
<p> 用户根据业务需求实现其中三个方法：<code>map()</code>、<code>setip()</code>、<code>cleanup()</code></p>
</li>
<li><p>Partitioner 分区</p>
<ol>
<li>有默认实现 <code>HashPartitioner</code>，逻辑是根据key的哈希值和<code>numReduces</code>来返回一个分区号；<code>key.hashCode()&amp;Integer.MAX_VALUE%numReduces</code></li>
<li>如果业务上有特变的需求，可以自定义分区。</li>
</ol>
</li>
<li><p>Comparable 排序</p>
<ol>
<li>当我们用自定义的对象作为key来输出时，就必须要是实现<code>WritableComparable</code>接口，重写其中的<code>compareTo()</code>方法</li>
<li>部分排序：对最终输出的每一个文件进行内部排序</li>
<li>全排序：对所有数据进行排序，通常只有一个Reduce</li>
<li>二次排序：排序的条件有两个</li>
</ol>
</li>
<li><p>Combiner 合并</p>
<p> Combiner 合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果</p>
</li>
<li><p>逻辑处理接口：Reducer</p>
<p> 用户根据业务需求实现其中三个方法：<code>reduce()</code>、<code>setup()</code>、<code>cleanup()</code></p>
</li>
<li><p>输出数据接口：OutputFormat</p>
<ol>
<li>默认实现类时<code>TextOutputFormat</code>，功能逻辑时：将每一个KV对，向目标文本文件输出一行</li>
<li>用户还可以自定义<code>OutputFormat</code></li>
</ol>
</li>
</ol>
<h1 id="四、Hadoop数据压缩"><a href="#四、Hadoop数据压缩" class="headerlink" title="四、Hadoop数据压缩"></a>四、Hadoop数据压缩</h1><h2 id="4-1、概述"><a href="#4-1、概述" class="headerlink" title="4.1、概述"></a>4.1、概述</h2><ol>
<li><p>压缩的好处和坏处</p>
<p> 压缩的优点：以减少磁盘IO、减少磁盘存储空间。</p>
<p> 压缩的缺点：增加CPU开销。</p>
</li>
<li><p>压缩原则</p>
<ol>
<li>运算密集型的Job，少用压缩</li>
<li>IO密集型的Job，多用压缩</li>
</ol>
</li>
</ol>
<h2 id="4-2-MR支持的压缩编码"><a href="#4-2-MR支持的压缩编码" class="headerlink" title="4.2 MR支持的压缩编码"></a>4.2 MR支持的压缩编码</h2><ol>
<li><p>压缩算法对比介绍</p>
<p> | 压缩格式 | hadoop自带？                                | 算法    | 文件扩展名 | 是否可切片                        | 换成压缩格式后。原来的程序是否需要修改                       |<br> | ———— | —————————————————————- | ———- | ————— | ————————————————- | —————————————————————————————— |<br> | DEFLATE  | 是，直接使用                                | DEFLATE | .deflate   | 否                                | 和文本处理一样，不需要修改                                   |<br> | Gzip     | 是，直接使用                                | DEFLATE | .gz        | 否                                | 和文本处理一样，不需要修改                                   |<br> | bzip2    | 是，直接使用                                | bzip2   | .bz2       | <span style="color:red">是</span> | 和文本处理一样，不需要修改                                   |<br> | LZO      | <span style="color:red">否，需要安装</span> | LZO     | .lzo       | <span style="color:red">是</span> | <span style="color:red">需要建索引，还需要指定输入格式</span> |<br> | Snappy   | 是，直接使用                                | Snappy  | .snappy    | 否                                | 和文本处理一样，不需要修改                                   |</p>
</li>
<li><p>压缩性能的比较</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820155328626.png" alt="image-20220820155328626"></p>
</li>
<li><p>压缩方式选择</p>
<p> 压缩方式选择时重点考虑：</p>
<ul>
<li>压缩/解压缩速度</li>
<li>压缩率（压缩后存储大小）</li>
<li>压缩后是否 可以支持切片。</li>
</ul>
</li>
</ol>
<h3 id="4-3-1、Gzip压缩"><a href="#4-3-1、Gzip压缩" class="headerlink" title="4.3.1、Gzip压缩"></a>4.3.1、Gzip压缩</h3><p>优点：压缩率比较高；</p>
<p>缺点：不支持Split；压缩/解压速度一般；</p>
<h3 id="4-3-2、Bzip2压缩"><a href="#4-3-2、Bzip2压缩" class="headerlink" title="4.3.2、Bzip2压缩"></a>4.3.2、Bzip2压缩</h3><p>优点：压缩率高；支持Split；</p>
<p>缺点：压缩/解压速度慢</p>
<h3 id="4-3-3-Lzo压缩"><a href="#4-3-3-Lzo压缩" class="headerlink" title="4.3.3 Lzo压缩"></a>4.3.3 Lzo压缩</h3><p>优点：压缩/解压速度比较块；支持Split；</p>
<p>缺点：压缩率一般；想支持切片需要额外创建索引</p>
<h3 id="4-3-4-Snappy压缩"><a href="#4-3-4-Snappy压缩" class="headerlink" title="4.3.4 Snappy压缩"></a>4.3.4 Snappy压缩</h3><p>优点：压缩饿解压速度块；</p>
<p>缺点：不支持Split；压缩率一般</p>
<h3 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h3><blockquote>
<p>压缩可以在MapReduce作用的任意阶段启用</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820155853410.png" alt="image-20220820155853410"></p>
<h2 id="4-4、压缩参数配置"><a href="#4-4、压缩参数配置" class="headerlink" title="4.4、压缩参数配置"></a>4.4、压缩参数配置</h2><ol>
<li><p>为了支持多种压缩/解压缩算法，Hadoop 引入了编码/解码器</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820160043206.png" alt="image-20220820160043206"></p>
</li>
<li><p>要在Hadoop中启用压缩，可以配置如下参数</p>
<p> | 参数                                                         | 默认值                                                       | 阶段                                          | 建议                                                         |<br> | —————————————————————————————— | —————————————————————————————— | ——————————————————————- | —————————————————————————————— |<br> | <span style="color:blue">io.compression.codecs（在 core-site.xml 中配置）</span> | <span style="color:blue">无，这个需要在命令行输入<code>hadoop checknative</code> 查看</span> | <span style="color:blue">输入压缩</span>      | <span style="color:blue">Hadoop 使用文件扩展 名判断是否支持某种 编解码器</span> |<br> | <span style="color:pink">mapreduce.map.output.compress（在 mapred-site.xml 中配置）</span> | <span style="color:pink">false</span>                        | <span style="color:pink">mapper 输出</span>   | <span style="color:pink">这个参数设为 true 启 用压缩</span>  |<br> | <span style="color:pink">mapreduce.map.output.compress.codec（在 mapred-site.xml 中配置）</span> | <span style="color:pink">org.apache.hadoop.io.com press.DefaultCodec</span> | <span style="color:pink">mapper 输出</span>   | <span style="color:pink">企业多使用 LZO 或 Snappy 编解码器在此 阶段压缩数据</span> |<br> | <span style="color:brown">mapreduce.output.fileoutputformat.compress（在 mapred-site.xml 中配置）</span> | <span style="color:brown">false</span>                       | <span style="color:brown">reducer 输出</span> | <span style="color:brown">这个参数设为 true 启 用压缩</span> |<br> | <span style="color:brown">mapreduce.output.fileoutputformat.compress.codec（在 mapred-site.xml 中配置）</span> | <span style="color:brown">org.apache.hadoop.io.com press.DefaultCodec</span> | <span style="color:brown">reducer 输出</span> | <span style="color:brown">使用标准工具或者编 解码器，如 gzip 和 bzip2</span> |</p>
</li>
</ol>
<h2 id="4-5、压缩案例实操"><a href="#4-5、压缩案例实操" class="headerlink" title="4.5、压缩案例实操"></a>4.5、压缩案例实操</h2><blockquote>
<p>以最简单的WordCount为例</p>
</blockquote>
<h3 id="4-5-1、Map输出端采用压缩"><a href="#4-5-1、Map输出端采用压缩" class="headerlink" title="4.5.1、Map输出端采用压缩"></a>4.5.1、Map输出端采用压缩</h3><p>即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<ol>
<li><p>提供的 Hadoop 源码支持的压缩格式有：BZip2Codec、DefaultCodec</p>
<p> 修改WordCountDriver类，添加20-23行代码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启 map 端输出压缩</span></span><br><span class="line">configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 设置 map 端输出压缩方式</span></span><br><span class="line">configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启 map 端输出压缩</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 设置 map 端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line">        </span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class); <span class="comment">// 关联 Mapper</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class); <span class="comment">// 关联 Reducer</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 设置 Mapper 输出的 kv 类型 ,注意有Map，没有map的是第5步调用的方法</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello_output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">false</span>);<span class="comment">// 参数true表示需要获取job的工作信息，false不用</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据结果返回值，如果result == true（成功）返回0，如果失败返回1</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Mapper 保持不变</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 输入：</span></span><br><span class="line"><span class="comment"> *      KEY ： LongWritable  // 第几行</span></span><br><span class="line"><span class="comment"> *      VALUE ： Text        // 该行的内容</span></span><br><span class="line"><span class="comment"> * 输出：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outKEY</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outVALUE</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>); <span class="comment">// 一个单词数量是 1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写map方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 输入数据的KEY</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 输入数据的VALUE</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 充当上下文，进行map和reduce之间的联络</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行 zyr zyr zyr</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割 [zyr,zyr,zyr]</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">// 封装outKEY</span></span><br><span class="line">            outKEY.set(word);</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(outKEY,outVALUE);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reducer 保持不变</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里和Mapper中的输入输出刚好相反</span></span><br><span class="line"><span class="comment"> * 输入：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> * 输出：</span></span><br><span class="line"><span class="comment"> *      KEY ： Text          // 被统计的单词</span></span><br><span class="line"><span class="comment"> *      VALUE ： IntWritable // 该单词的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outValue</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 重写reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   输入的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values    输入的value，是一个可迭代对象(不是迭代器，它时一个集合，如果想要遍历它，需要先用value.iterator()得到一个迭代器，或者增强for循环，集合的遍历方式都可)</span></span><br><span class="line"><span class="comment">     *      例如zyr zyr zyr,被mapper弄成了三个，所以传入的K-V实际上是三组 zyr-1。但是Reducer在接收数据时会转化为：zyr-(1,1,1)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   上下文作用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 累加  zyr-(1,1,1)</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key,outValue);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>效果和之前一样，因为设置的压缩式Mapper输出压缩，到了Reducer又加压缩了。</p>
</li>
</ol>
<h3 id="4-5-2、Reduce输出端采用压缩"><a href="#4-5-2、Reduce输出端采用压缩" class="headerlink" title="4.5.2、Reduce输出端采用压缩"></a>4.5.2、Reduce输出端采用压缩</h3><p>基于WordCount案例吃力</p>
<ol>
<li><p>修改WordCountDriver驱动类,添加46-49行的内容</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取配置信息以及获取 job 对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启 map 端输出压缩</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 设置 map 端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 关联本 Driver 程序的 jar</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 关联 Mapper 和 Reducer 的 jar</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class); <span class="comment">// 关联 Mapper</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class); <span class="comment">// 关联 Reducer</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 设置 Mapper 输出的 kv 类型 ,注意有Map，没有map的是第5步调用的方法</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 设置最终输出 kv 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;D:\\桌面\\hello_output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 reduce 端输出压缩开启</span></span><br><span class="line">        FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 设置压缩的方式</span></span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line"><span class="comment">// 		  FileOutputFormat.setOutputCompressorClass(job,DefaultCodec.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7. 提交 job</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">result</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">false</span>);<span class="comment">// 参数true表示需要获取job的工作信息，false不用</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据结果返回值，如果result == true（成功）返回0，如果失败返回1</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Mapper和Reducer保持不变</p>
</li>
<li><p>效果：</p>
<p> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/image-20220820162626137.png" alt="image-20220820162626137" style="zoom:67%;" /></p>
</li>
</ol>
<p>==各个阶段的压缩方式不同也影响结果==</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yongruizhang.cn">YongRui Zhang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yongruizhang.cn/posts/3156194925.html">http://yongruizhang.cn/posts/3156194925.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yongruizhang.cn" target="_blank">秋白's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/Linux/">Linux</a></div><div class="post_share"><div class="social-share" data-image="https://th.bing.com/th/id/R.d60c7923a45b06826278a8d3fa6cd1cc?rik=N6M9fQqCw7Ijsg&amp;riu=http%3a%2f%2ferrequeerre.es%2fwp-content%2fuploads%2f2017%2f08%2fHadoop-Map-Reduce.png&amp;ehk=4zYF4J3scSG5mcQ%2fFQqylAoWYreGhEr7IFpmCu8lXnc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/weixin.jpeg" target="_blank"><img class="post-qr-code-img" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/weixin.jpeg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/alipay.jpeg" target="_blank"><img class="post-qr-code-img" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/alipay.jpeg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/3213899550.html" title="Redis笔记"><img class="cover" src="https://th.bing.com/th/id/OIP.2Y3_2LXTMR2wXeJebPkGKAHaEf?pid=ImgDet&amp;rs=1" onerror="onerror=null;src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Redis笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/1767336200.html" title="Yarn"><img class="cover" src="https://www.leixue.com/uploads/2020/09/Apache-Hadoop-YARN.png!760" onerror="onerror=null;src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Yarn</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/1547067935.html" title="HDFS"><img class="cover" src="https://th.bing.com/th/id/OIP.pyCJhEElTwE2DtLJoh_yGgHaHa?pid=ImgDet&rs=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-14</div><div class="title">HDFS</div></div></a></div><div><a href="/posts/1767336200.html" title="Yarn"><img class="cover" src="https://www.leixue.com/uploads/2020/09/Apache-Hadoop-YARN.png!760" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-14</div><div class="title">Yarn</div></div></a></div><div><a href="/posts/4130790367.html" title="hadoop常用端口号"><img class="cover" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/Big-Data-Hadoop.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-29</div><div class="title">hadoop常用端口号</div></div></a></div><div><a href="/posts/4177218757.html" title="hadoop集群搭建指南"><img class="cover" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/hadoop_2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-29</div><div class="title">hadoop集群搭建指南</div></div></a></div><div><a href="/posts/1817748743.html" title="搭建HBase集群"><img class="cover" src="https://www.wenjiangs.com/wp-content/uploads/2020/03/hbase.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-25</div><div class="title">搭建HBase集群</div></div></a></div><div><a href="/posts/1141628095.html" title="搭建Hadoop环境"><img class="cover" src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/hadoop.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-28</div><div class="title">搭建Hadoop环境</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/favicon.png" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">秋白</div><div class="author-info__description">埋头学习 抬头恋爱</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">96</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">57</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yongruizhang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/yongruizhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:yongruizhang@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/367319940?spm_id_from=333.337.0.0" target="_blank" title="bilibili"><i class="fa-brands fa-bilibili"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><p style="text-indent:2em;">我是数据科学与大数据专业本科生。<br/></p><p style="text-indent:2em;">目前主要学习大数据开发，数学建模，机器学习，算法。之前浅学过Java后端，有一点前端基础。<br/></p><p style="text-indent:2em;">为了您的体验，没有设置复制权限，您可以复制文章内容和代码来做笔记等，但您要发布文章请注明内容出处，请遵循开源原则🤝</p><p style="text-indent:2em;">如果文章内容有错误，或者版本等问题拜托在评论区留言，为后来人搭桥。感谢🙏</p><p style="text-indent:2em;">由于我是利用电脑端Safari浏览器写的相关样式，所以不同平台的效果可能不同，不推荐使用手机访问博客（我没怎么适配，可能会有些显示问题）。前端技术有限😅<br/></p></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A6%82%E8%BF%B0"><span class="toc-text">一、概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E3%80%81%E5%AE%9A%E4%B9%89"><span class="toc-text">1.1 、定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E3%80%81%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">1.2、优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1%E3%80%81%E4%BC%98%E7%82%B9"><span class="toc-text">1.2.1、优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2%E3%80%81%E7%BC%BA%E7%82%B9"><span class="toc-text">1.2.2、缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3%E3%80%81%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1.3、核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4%E3%80%81MapReduce-%E8%BF%9B%E7%A8%8B"><span class="toc-text">1.4、MapReduce 进程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5%E3%80%81%E5%AE%98%E6%96%B9-WordCount-%E6%BA%90%E7%A0%81"><span class="toc-text">1.5、官方 WordCount 源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-6%E3%80%81%E9%87%8D%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%E7%B1%BB%E5%9E%8B"><span class="toc-text">1.6、重用数据序列化类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-7%E3%80%81MapReduce-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-text">1.7、MapReduce 编程规范</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81Mapper"><span class="toc-text">1、Mapper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81Reducer"><span class="toc-text">2、Reducer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81Driver"><span class="toc-text">3、Driver</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-8%E3%80%81WordCount%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">1.8、WordCount案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95"><span class="toc-text">本地测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-9%E3%80%81%E6%8F%90%E4%BA%A4%E5%88%B0%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95"><span class="toc-text">1.9、提交到集群测试</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Hadoop%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">二、Hadoop序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E3%80%81%E6%A6%82%E5%BF%B5"><span class="toc-text">2.1、概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89bean%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3%EF%BC%88Writable%EF%BC%89"><span class="toc-text">2.2、自定义bean对象实现序列化接口（Writable）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3%E3%80%81%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">2.3、序列化案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82"><span class="toc-text">需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-text">需求分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99-MapReduce-%E7%A8%8B%E5%BA%8F"><span class="toc-text">编写 MapReduce 程序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81MapReduce-%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86"><span class="toc-text">三、MapReduce 框架原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81InputFormat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5"><span class="toc-text">3.1、InputFormat 数据输入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E5%88%87%E7%89%87%E4%BA%8EMapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6"><span class="toc-text">3.1.1 切片于MapTask并行度决定机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2%E3%80%81Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="toc-text">3.1.2、Job提交流程源码详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3%E3%80%81-FileInputFormat-%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E5%92%8C%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-text">3.1.3、 FileInputFormat 切片源码和切片机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4%E3%80%81TextInputFormat"><span class="toc-text">3.1.4、TextInputFormat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5%E3%80%81CombineTextInputFormat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-text">3.1.5、CombineTextInputFormat 切片机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81MapReduce-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">3.2、MapReduce 工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E3%80%81Shuffle-%E6%9C%BA%E5%88%B6"><span class="toc-text">3.3、Shuffle 机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1%E3%80%81Shuffle%E6%9C%BA%E5%88%B6"><span class="toc-text">3.3.1、Shuffle机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2%E3%80%81Partition-%E5%88%86%E5%8C%BA"><span class="toc-text">3.3.2、Partition 分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3%E3%80%81-Partition-%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">3.3.3、 Partition 分区案例实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4%E3%80%81WritableComparable-%E6%8E%92%E5%BA%8F"><span class="toc-text">3.3.4、WritableComparable 排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-5%E3%80%81WritableComparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%88%E5%85%A8%E6%8E%92%E5%BA%8F%EF%BC%89"><span class="toc-text">3.3.5、WritableComparable 排序案例实操（全排序）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-6%E3%80%81WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%EF%BC%88%E5%8C%BA%E5%86%85%E6%8E%92%E5%BA%8F%EF%BC%89"><span class="toc-text">3.3.6、WritableComparable排序案例实操（区内排序）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-7%E3%80%81Combiner-%E5%90%88%E5%B9%B6"><span class="toc-text">3.3.7、Combiner 合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-8%E3%80%81Combiner%E5%90%88%E5%B9%B6%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">3.3.8、Combiner合并案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E3%80%81OutPutFormat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA"><span class="toc-text">3.4、OutPutFormat 数据输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1%E3%80%81OutputFormat-%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0%E7%B1%BB"><span class="toc-text">3.4.1、OutputFormat 接口实现类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">3.4.2、自定义OutputFormat案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5%E3%80%81MapReduce-%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-text">3.5、MapReduce 内核源码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1%E3%80%81MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-text">3.5.1、MapTask工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2%E3%80%81ReduceTask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-text">3.5.2、ReduceTask 工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-3%E3%80%81ReduceTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6"><span class="toc-text">3.5.3、ReduceTask并行度决定机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6%E3%80%81Join%E5%BA%94%E7%94%A8"><span class="toc-text">3.6、Join应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-1%E3%80%81Reduce-Join"><span class="toc-text">3.6.1、Reduce Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-2%E3%80%81Reduce-Join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">3.6.2、Reduce Join 案例实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-3%E3%80%81Map-Join"><span class="toc-text">3.6.3、Map Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-4%E3%80%81Map-Join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">3.6.4、Map Join 案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7%E3%80%81%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%88ETL%EF%BC%89"><span class="toc-text">3.7、数据清洗（ETL）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8%E3%80%81MapReduce%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93"><span class="toc-text">3.8、MapReduce开发总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="toc-text">四、Hadoop数据压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81%E6%A6%82%E8%BF%B0"><span class="toc-text">4.1、概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-MR%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="toc-text">4.2 MR支持的压缩编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E3%80%81Gzip%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.3.1、Gzip压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E3%80%81Bzip2%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.3.2、Bzip2压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-Lzo%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.3.3 Lzo压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-Snappy%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.3.4 Snappy压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9"><span class="toc-text">压缩位置选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4%E3%80%81%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-text">4.4、压缩参数配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5%E3%80%81%E5%8E%8B%E7%BC%A9%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-text">4.5、压缩案例实操</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1%E3%80%81Map%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.5.1、Map输出端采用压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2%E3%80%81Reduce%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9"><span class="toc-text">4.5.2、Reduce输出端采用压缩</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/67485572.html" title="Vue组件化编程"><img src="https://ts1.cn.mm.bing.net/th/id/R-C.7a131031711c74fa64145ef288032dbe?rik=cZtWL1%2fgyDKdhg&amp;riu=http%3a%2f%2fwww.lgwimonday.cn%2fcosyblog%2fwordpress%2fwp-content%2fuploads%2f2019%2f07%2ftimgvue.jpg&amp;ehk=Co5mIoKNa2j55UOcR7sOyNZdvRYGYIlrK2aCMBur1cQ%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="Vue组件化编程"/></a><div class="content"><a class="title" href="/posts/67485572.html" title="Vue组件化编程">Vue组件化编程</a><time datetime="2023-06-01T00:53:13.000Z" title="发表于 2023-06-01 08:53:13">2023-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2596601004.html" title="Vue核心基础"><img src="https://tse3-mm.cn.bing.net/th/id/OIP-C.7cuBHJ7peZ6X-aOuCG7w5gHaFL?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="Vue核心基础"/></a><div class="content"><a class="title" href="/posts/2596601004.html" title="Vue核心基础">Vue核心基础</a><time datetime="2023-05-31T04:59:36.000Z" title="发表于 2023-05-31 12:59:36">2023-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2421785022.html" title="JetBrains（IDEA、PyCharm等）下载教程"><img src="https://devclass.com/wp-content/uploads/2018/12/jetbrains-variant-4.png" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="JetBrains（IDEA、PyCharm等）下载教程"/></a><div class="content"><a class="title" href="/posts/2421785022.html" title="JetBrains（IDEA、PyCharm等）下载教程">JetBrains（IDEA、PyCharm等）下载教程</a><time datetime="2023-05-27T04:21:09.000Z" title="发表于 2023-05-27 12:21:09">2023-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3297135020.html" title="搭建Hadoop HA(高可用)集群"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.uEYE9pVH52RNgMcLUl1tYgHaEL?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="搭建Hadoop HA(高可用)集群"/></a><div class="content"><a class="title" href="/posts/3297135020.html" title="搭建Hadoop HA(高可用)集群">搭建Hadoop HA(高可用)集群</a><time datetime="2023-05-19T05:18:56.000Z" title="发表于 2023-05-19 13:18:56">2023-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3213899550.html" title="Redis笔记"><img src="https://th.bing.com/th/id/OIP.2Y3_2LXTMR2wXeJebPkGKAHaEf?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='https://fastly.jsdelivr.net/npm/yongruizhang-blogimg@latest/404.jpg'" alt="Redis笔记"/></a><div class="content"><a class="title" href="/posts/3213899550.html" title="Redis笔记">Redis笔记</a><time datetime="2023-05-14T14:14:29.000Z" title="发表于 2023-05-14 22:14:29">2023-05-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By 秋白</div><div class="footer_custom_text">蜀ICP备2023012316号-1<br/>埋头学习 抬头恋爱<br/>本博客为个人博客，内容仅供学习参考<br/>如果文章帮到您了，或者您觉得我比较有趣请给我评论或打赏～</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://blog-comments-phi-eight.vercel.app',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://blog-comments-phi-eight.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async src="https://cdn.jsdelivr.net/gh/zhheo/twikoo@dev/dist/twikoo.all.min.js"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "52b81d8c-40b2-40bc-bd82-3bceb709cf4b";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (true) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>